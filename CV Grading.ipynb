{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install pandas openpyxl transformers torch scikit-learn\n",
        "\n",
        "import pdfplumber\n",
        "import re\n",
        "import os\n",
        "import openpyxl\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW # Added AdamW\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader # Added imports"
      ],
      "metadata": {
        "id": "FDkpuoAQxIsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Function to clean the extracted text\n",
        "def clean_text(text):\n",
        "    # Remove multiple spaces, newlines, etc.\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Collapse multiple whitespaces into one\n",
        "    text = text.strip()  # Remove leading/trailing spaces\n",
        "    return text\n",
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        # Extract text from all pages\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Function to save extracted data into an Excel file\n",
        "def save_to_excel(data, output_path):\n",
        "    # Create a new Excel workbook and sheet\n",
        "    wb = openpyxl.Workbook()\n",
        "    ws = wb.active\n",
        "    ws.title = \"CV Texts\"\n",
        "\n",
        "    # Write the header\n",
        "    ws.append([\"File Name\", \"Extracted Text\"])  # Column headers\n",
        "\n",
        "    # Write each PDF's name and extracted text into a row\n",
        "    for file_name, text in data.items():\n",
        "        ws.append([file_name, text])\n",
        "\n",
        "    # Save the workbook\n",
        "    wb.save(output_path)\n",
        "\n",
        "# Main function to process all PDF files in a folder\n",
        "def process_pdfs_in_folder(folder_path, excel_output_path):\n",
        "    extracted_data = {}\n",
        "\n",
        "    # Loop over all files in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.pdf'):  # Process only PDF files\n",
        "            pdf_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            # Extract text from the current PDF file\n",
        "            extracted_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "            # Clean the extracted text\n",
        "            cleaned_text = clean_text(extracted_text)\n",
        "\n",
        "            # Store the extracted data (file name as key, cleaned text as value)\n",
        "            extracted_data[filename] = cleaned_text\n",
        "\n",
        "    # Save all extracted data to an Excel file\n",
        "    save_to_excel(extracted_data, excel_output_path)\n",
        "    print(f\"Text extracted and saved to {excel_output_path}\")\n",
        "\n",
        "# Specify your folder path containing the PDFs and the output Excel file path\n",
        "folder_path = '/content/CV/'\n",
        "excel_output_path = '/content/CV Text.xlsx'  # Output path for the Excel file\n",
        "\n",
        "# Run the process\n",
        "process_pdfs_in_folder(folder_path, excel_output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnM1slEzvcbm",
        "outputId": "68317022-31a4-46e2-a9b2-f0dfa8567616"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Text extracted and saved to /content/CV Text.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def load_and_fine_tune_model(train_texts, train_labels, epochs=3):\n",
        "    model_name = 'bert-base-uncased'\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1) # num_labels should match the number of classes in your dataset\n",
        "\n",
        "    # Prepare training data\n",
        "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    # Convert train_labels to numerical labels if they are strings\n",
        "    # Assuming you want to assign a unique numerical label to each file name\n",
        "    unique_labels = list(set(train_labels))\n",
        "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "    numerical_labels = [label_map[label] for label in train_labels]\n",
        "\n",
        "    train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
        "                                  torch.tensor(train_encodings['attention_mask']),\n",
        "                                  torch.tensor(numerical_labels)) # Use numerical labels here\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) # Adjust batch_size as needed\n",
        "\n",
        "    # Fine-tune the model\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels.unsqueeze(1).float()) # Adjust labels shape if necessary\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "# Load Excel data into a pandas DataFrame\n",
        "def load_excel_data(excel_path):\n",
        "    df = pd.read_excel(excel_path)\n",
        "    return df\n",
        "\n",
        "# Function to load a pre-trained BERT model\n",
        "def load_model():\n",
        "    model_name = 'bert-base-uncased'  # You can also use other models like 'distilbert-base-uncased'\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    return tokenizer, model\n",
        "\n",
        "# Function to encode text using BERT tokenizer\n",
        "def encode_text(text, tokenizer):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    return inputs\n",
        "\n",
        "# Function to get embeddings for the CVs and job descriptions\n",
        "def get_embeddings(texts, tokenizer, model):\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            inputs = encode_text(text, tokenizer)\n",
        "            outputs = model(**inputs)\n",
        "            # Reshape the embeddings to 2D by taking the mean of the first dimension\n",
        "            # This assumes the first dimension represents different aspects of the embedding\n",
        "            # and we want to average them to get a single representation\n",
        "            embeddings.append(outputs.logits.detach().numpy().mean(axis=1))\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Function to rank the CVs based on similarity to a job description\n",
        "def rank_cvs(job_desc, cv_texts, tokenizer, model):\n",
        "    # Get the embeddings for the job description and CVs\n",
        "    job_desc_embedding = get_embeddings([job_desc], tokenizer, model)\n",
        "    cv_embeddings = get_embeddings(cv_texts, tokenizer, model)\n",
        "\n",
        "    # Compute cosine similarity between the job description and each CV\n",
        "    similarities = cosine_similarity(job_desc_embedding, cv_embeddings)\n",
        "    return similarities[0]\n",
        "\n",
        "\n",
        "# Main function to perform CV ranking\n",
        "def rank_cvs_in_excel(excel_path, job_desc):\n",
        "    # Load the Excel file\n",
        "    df = load_excel_data(excel_path)\n",
        "\n",
        "    # Extract the CV texts from the Excel file\n",
        "    cv_texts = df['Extracted Text'].tolist()\n",
        "\n",
        "    # Extract labels for fine-tuning (if available)\n",
        "    cv_labels = df['File Name'].tolist() if 'File Name' in df.columns else None\n",
        "\n",
        "    # Load and fine-tune the model or load the pre-trained model\n",
        "    if cv_labels:\n",
        "        tokenizer, model = load_and_fine_tune_model(cv_texts, cv_labels)\n",
        "    else:\n",
        "        tokenizer, model = load_model()\n",
        "\n",
        "    # Rank the CVs based on the job description (using the loaded/fine-tuned model)\n",
        "    similarities = rank_cvs(job_desc, cv_texts, tokenizer, model)\n",
        "\n",
        "    # Add the similarity scores to the DataFrame\n",
        "    df['Similarity Score'] = similarities\n",
        "\n",
        "    # Sort the CVs by similarity score (higher score = more relevant)\n",
        "    ranked_df = df.sort_values(by='Similarity Score', ascending=False)\n",
        "\n",
        "    # Save the ranked CVs to a new Excel file\n",
        "    ranked_df.to_excel('ranked_cvs.xlsx', index=False)\n",
        "\n",
        "    # Print out the top-ranked CVs\n",
        "    print(\"Top ranked CVs:\")\n",
        "    print(ranked_df[['File Name','Extracted Text', 'Similarity Score']].head())\n",
        "    return model\n",
        "# Example usage\n",
        "job_desc = \"\"\"\n",
        "ob Summary:\n",
        "We are seeking a motivated and detail-oriented member to join our team. In this role, you will bridge the gap between IT and business operations, using data analytics to assess processes, determine requirements, and deliver data-driven recommendations to stakeholders. This is an excellent opportunity for recent graduates to develop their skills and contribute to meaningful projects.\n",
        "\n",
        "Key Responsibilities:\n",
        "Requirement Gathering: Work closely with stakeholders to understand and document business needs and translate them into technical requirements.\n",
        "System Analysis and Design: Analyze existing systems for improvement, propose new system processes, and assist in designing solutions that meet business needs.\n",
        "Implementation and Support: Assist in the implementation of new systems or updates to existing systems. Provide support and troubleshooting for system issues.\n",
        "Data Analysis: Use data analytics tools to gather and analyze data, generate reports, and provide insights to support decision-making.\n",
        "Documentation: Develop and maintain system documentation, including user manuals and technical guides.\n",
        "Testing and Quality Assurance: Participate in system testing to ensure solutions meet business requirements and are free of defects.\n",
        "Training: Assist in training end-users on new systems and processes.\n",
        "Continuous Improvement: Stay updated with the latest technology trends and suggest improvements to enhance system efficiency and effectiveness.\n",
        "Qualifications:\n",
        "Education: Bachelor’s degree in Business Administration, Information Technology, Computer Science, or a related field.\n",
        "Technical Skills: Basic understanding of databases, networking, and software development. Familiarity with data analysis tools like Excel, SQL, or Tableau is a plus.\n",
        "Analytical Skills: Strong analytical and problem-solving skills to interpret data and develop actionable insights.\n",
        "Communication Skills: Excellent written and verbal communication skills to effectively interact with stakeholders and team members.\n",
        "Teamwork: Ability to work collaboratively in a team-oriented environment.\n",
        "Attention to Detail: High level of accuracy and attention to detail in documentation and analysis.\n",
        "Benefits:\n",
        "Competitive salary and benefits package\n",
        "Professional development opportunities\n",
        "Mentorship and training programs\n",
        "Collaborative and supportive work environment\n",
        "Opportunities for career advancement\n",
        "\"\"\"\n",
        "\n",
        "excel_path = '/content/Training.xlsx'  # Path to your Excel file\n",
        "\n",
        "# Rank the CVs in the Excel file based on the provided job description\n",
        "model = rank_cvs_in_excel(excel_path, job_desc)  # model is now assigned\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-OU5uym1GKn",
        "outputId": "fda69b7b-cb46-4937-ca0f-b3418e853219"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top ranked CVs:\n",
            "  File Name                                     Extracted Text  \\\n",
            "0       Bad  Name: Emily Davis Contact Information: • Phone...   \n",
            "1      Good  Name: Sarah Johnson Contact Information: • Pho...   \n",
            "2      Good  Name: Alex Turner Contact Information: • Phone...   \n",
            "3       Bad  Emily Davis Contact Information: • Phone: +61 ...   \n",
            "4      Good  Sarah Johnson Contact Information: • Phone: +6...   \n",
            "\n",
            "   Similarity Score  \n",
            "0               1.0  \n",
            "1               1.0  \n",
            "2               1.0  \n",
            "3               1.0  \n",
            "4               1.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_new_cvs(job_desc, cv_texts, model_path='fine_tuned_model'):\n",
        "    \"\"\"Ranks new CVs using a saved fine-tuned model.\"\"\"\n",
        "\n",
        "    # Load the fine-tuned model and tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Get embeddings and rank CVs\n",
        "    similarities = rank_cvs(job_desc, cv_texts, tokenizer, model)\n",
        "    return similarities\n",
        "\n",
        "# Example usage:\n",
        "# 1. Save the fine-tuned model:\n",
        "# Assuming you have already trained the model and assigned it to the `model` variable\n",
        "model.save_pretrained('fine_tuned_model')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer.save_pretrained('fine_tuned_model')\n",
        "# 2. Load new CV texts:\n",
        "new_cv_texts = [\n",
        "    \"I have strong experience in Python and machine learning.\",\n",
        "    \"I am proficient in data analysis and SQL.\"\n",
        "]\n",
        "\n",
        "# 3. Rank the new CVs:\n",
        "similarities = rank_new_cvs(job_desc, new_cv_texts)\n",
        "\n",
        "# 4. Process the results (e.g., print the rankings):\n",
        "for i, similarity in enumerate(similarities):\n",
        "    print(f\"CV {i + 1}: Similarity Score = {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weQrtWW-2Nxo",
        "outputId": "474cbfaf-9f06-4623-b715-73bf93745054"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV 1: Similarity Score = 1.0\n",
            "CV 2: Similarity Score = 1.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "preprocessing_layers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}